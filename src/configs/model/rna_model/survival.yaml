attention_probs_dropout_prob: 0.0
decoder_hidden_size: 128
decoder_intermediate_size: 512
decoder_num_attention_heads: 4
decoder_num_hidden_layers: 2
hidden_act: "gelu"
hidden_dropout_prob: 0.0
hidden_size: 256 # maybe 384?
rna_size: 16384
initializer_range: 0.02
intermediate_size: 1024
layer_norm_eps: 1e-12
mask_ratio: 0.5
model_type: "vit_mae"
norm_pix_loss: false
num_attention_heads: 4
num_channels: 1
num_hidden_layers: 4
patch_size: 512
qkv_bias: true
is_load_pretrained: false
pretrained_model_name: ""
pretrained_model_path: ""
output_dim: 20